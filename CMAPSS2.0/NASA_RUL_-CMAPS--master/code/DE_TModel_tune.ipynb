{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Get best parameters for each dataset. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "\n",
    "from ann_framework.data_handlers.data_handler_CMAPSS import CMAPSSDataHandler\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelRegression\n",
    "from ann_framework import aux_functions\n",
    "\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()  #Clear the previous tensorflow graph\n",
    "\n",
    "l2_lambda_regularization = 0.20\n",
    "l1_lambda_regularization = 0.10\n",
    "\n",
    "def RULmodel_SN(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected as per CNN paper\n",
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 125\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "dHandler_cmaps = CMAPSSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "#dHandler_cmaps.load_data(verbose=1, cross_validation_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create theTunable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#min_max_scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(shape, model_type='ann'):\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0, beta_1=0.5)\n",
    "    lossFunction = \"mean_squared_error\"\n",
    "    metrics = [\"mse\"]\n",
    "    model = None\n",
    "\n",
    "    #Create and compile the models\n",
    "\n",
    "    if model_type=='ann':\n",
    "        model = RULmodel_SN(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    elif model_type=='lstm':\n",
    "        \"\"\"nFeatures = len(selected_features)\n",
    "        shapeLSTM = (window_size, nFeatures)\n",
    "        model = RULmodel_LSTM(shapeLSTM)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\"\"\"\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures = len(selected_features)\n",
    "shapeSN = nFeatures*window_size\n",
    "modelRULSN = get_compiled_model(shapeSN, model_type='ann')\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_SN_1', modelRULSN, lib_type='keras', data_handler=dHandler_cmaps)\n",
    "#tModel = SequenceTunableModelRegression('ModelRUL_LSTM_1', modelRULLSTM, lib_type='keras', data_handler=dHandler_cmaps)\n",
    "tModel.data_scaler = min_max_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define objective function\n",
    "\n",
    "Define the function that evaluates each set of data-related params and returns the RMSE as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_optmize_fun(x, tModel, verbose=0, epochs=250, saveToFile = None, iterations = 0):\n",
    "    \n",
    "    #Clear the previous tensorflow graph\n",
    "    K.clear_session()\n",
    "    \n",
    "    maxWindowSize = {'1':30, '2':20, '3':30, '4':18}\n",
    "    \n",
    "    #Extract the tunning variables from the input vector\n",
    "    #Round the values to the nearest integer since this implementation is for real numbers\n",
    "    x = x.astype(int)\n",
    "\n",
    "    #load the data using the selected parameters\n",
    "    tModel.data_handler.sequence_length = x[0]\n",
    "    #tModel.data_handler.sequence_length = maxWindowSize[datasetNumber]\n",
    "    tModel.data_handler.sequence_stride = x[1]\n",
    "    tModel.data_handler.max_rul = x[2]\n",
    "    \n",
    "    tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0, reload_data=True)\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Training model\")\n",
    "    \n",
    "    #Create new model\n",
    "    lrate = LearningRateScheduler(aux_functions.step_decay)\n",
    "    nFeatures = len(selected_features)\n",
    "    model_shape = nFeatures*tModel.data_handler.sequence_length\n",
    "    modelRULSN = get_compiled_model(model_shape, model_type='ann')\n",
    "    tModel.change_model('ModelRUL_SN_1', modelRULSN, 'keras')\n",
    "    tModel.epochs = 20\n",
    "    \n",
    "    #Train model\n",
    "    tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "    time = tModel.train_time\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Training time {}\".format(time))\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Assesing model performance\")\n",
    "    #Assess the model performance\n",
    "    tModel.evaluate_model([\"rmse\", \"rhs\"], round=2)\n",
    "    cScores = tModel.scores\n",
    "    rmse = cScores['rmse']\n",
    "    rhs = cScores['rhs']\n",
    "    #print(\"The score for this model is: {}\".format(rmse))\n",
    "    \n",
    "    print(x)\n",
    "    print(rmse)\n",
    "    print(rhs)\n",
    "    msgStr = \"The model variables are \" + str(x) + \"\\tThe scores are: [RMSE:{:.4f}, RHS:{:.4f}]\\n\".format(rmse, rhs)\n",
    "    print(msgStr)\n",
    "    row = x.tolist() + [rmse, rhs]\n",
    "    \n",
    "    if saveToFile is not None:\n",
    "        #print(msgStr)\n",
    "        writer = csv.writer(saveToFile)\n",
    "        #row = x.append(rmse)\n",
    "        #row = x.append(rhs)\n",
    "        writer.writerow(row)\n",
    "        #saveToFile.write(msgStr)\n",
    "    else:\n",
    "        print(row)\n",
    "    \n",
    "    #Return RMSE as the performance metric to steer the search\n",
    "    return rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the parameters for the NN using DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the parameters for the NN using DE\n",
    "\n",
    "#maxWindowSize = {'1':30, '2':20, '3':30, '4':18}\n",
    "maxWindowSize = {'1':30, '2':20} #Do it only for datasets 1 and 2\n",
    "totalTime = {'1':0, '2':0, '3':0, '4':0}\n",
    "results = {'1':0, '2':0, '3':0, '4':0}\n",
    "\n",
    "#datasetNumber = '1'\n",
    "\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "\n",
    "\n",
    "for datasetNumber in maxWindowSize:\n",
    "    \n",
    "    print(\"Tunning for dataset \"+datasetNumber)\n",
    "    file = open(\"results/MLP/intermediateResults_refactorized_\"+datasetNumber+\".csv\", \"w\")\n",
    "\n",
    "    windowSizeBounds = [1,maxWindowSize[datasetNumber]]\n",
    "    windowStrideBounds = [1,10]\n",
    "    constantRULBounds = [90,140]\n",
    "\n",
    "    bounds = [windowSizeBounds, windowStrideBounds, constantRULBounds]\n",
    "    #bounds = [windowStrideBounds, constantRULBounds] #Optimize only 2 variabes\n",
    "    \n",
    "    tModel.data_handler.change_dataset(datasetNumber)\n",
    "\n",
    "    startTime = time.clock()\n",
    "    tempResults = differential_evolution(nn_optmize_fun, bounds, \n",
    "                                     args=(tModel, 0, 20, file, 1),\n",
    "                                    strategy='best1bin', maxiter=30, popsize=4, disp=True, polish=False)\n",
    "    results[datasetNumber] = tempResults\n",
    "    endTime = time.clock()\n",
    "\n",
    "    file.close()\n",
    "    totalTime[datasetNumber] = endTime - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total time {}\".format(totalTime))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}