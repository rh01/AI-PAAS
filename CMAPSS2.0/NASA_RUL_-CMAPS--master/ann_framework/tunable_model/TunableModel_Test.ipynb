{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test notebook for tunable model. Tests for both classification and regression using Keras and Tensorflow\n",
    "\n",
    "We first import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D, LSTM\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "#Use the data handler to manage all the data preprocessing.\n",
    "from ann_framework.data_handlers.data_handler_CMAPSS import CMAPSSDataHandler\n",
    "from ann_framework.data_handlers.data_handler_MNIST import MNISTDataHandler\n",
    "\n",
    "#Import the tunable model classes\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelRegression, SequenceTunableModelClassification\n",
    "\n",
    "#Import aux functions\n",
    "import ann_framework.aux_functions as aux_functions\n",
    "\n",
    "\n",
    "l2_lambda_regularization = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define keras example models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_test_model_regression(input_shape, output_size=1):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal',\n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(output_size, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def keras_test_model_classification(input_shape, output_size=1):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(80, input_dim=input_shape, activation='tanh', kernel_initializer='glorot_normal', name='fc1'))\n",
    "    model.add(Dense(output_size, activation='softmax', kernel_initializer='glorot_normal', name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_keras_compiled_model(input_shape, model_type, output_size=1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.5)\n",
    "    \n",
    "    if model_type == \"regression\":\n",
    "        \n",
    "        #Regression model\n",
    "        lossFunction = \"mean_squared_error\"\n",
    "        metrics = [\"mse\"]\n",
    "        model = keras_test_model_regression(input_shape, output_size)\n",
    "    else:\n",
    "    \n",
    "        #Classification model\n",
    "        lossFunction = \"categorical_crossentropy\"\n",
    "        metrics = [\"accuracy\"]\n",
    "        model = keras_test_model_classification(input_shape, output_size)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tensorflow example models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(input_shape, output_shape):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None,input_shape), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=None, name=\"y\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tf_model_classification(X, output_size=1):\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 80, activation=tf.nn.tanh, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), name=\"fc1\")\n",
    "    logits = tf.layers.dense(A1, output_size, activation=None, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), name=\"fc2\")\n",
    "    y = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, y \n",
    "\n",
    "\n",
    "def tf_model_regression(X, output_size=1):\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 20, activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), \n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_lambda_regularization), name=\"fc1\")\n",
    "    y = tf.layers.dense(A1, output_size, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_lambda_regularization), name=\"out\")\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def get_tf_compiled_model(num_features, model_type, output_size=1):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X, y = create_placeholders(num_features, output_size)\n",
    "    \n",
    "    if model_type == \"regression\":\n",
    "        \n",
    "        #Regression model\n",
    "        y_pred = tf_model_regression(X, output_size)\n",
    "        cost = tf.losses.mean_squared_error(y_pred, y)\n",
    "    else:\n",
    "    \n",
    "        #Classification model\n",
    "        logits, y_pred = tf_model_classification(X, output_size)\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
    "    \n",
    "    reg_cost = tf.losses.get_regularization_loss()\n",
    "    total_cost = cost + reg_cost\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.5).minimize(total_cost)\n",
    "    \n",
    "    return {'X_placeholder':X, 'y_placeholder':y, 'y_pred':y_pred, 'cost':cost, 'total_cost':total_cost, 'optimizer':optimizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load CMAPSS data handler\"\"\"\n",
    "\n",
    "#Selected as per CNN paper\n",
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../../NASA_RUL_(CMAPS)/CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 125\n",
    "\n",
    "dHandler_cmaps = CMAPSSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "\"\"\"Load MNIST data handler\"\"\"\n",
    "dHandler_mnist = MNISTDataHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TunableModel with regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(selected_features)\n",
    "shape = len(selected_features)*window_size\n",
    "\n",
    "#Get keras model\n",
    "model = get_keras_compiled_model(shape, \"regression\", output_size=1)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_Keras', model, lib_type='keras', data_handler=dHandler_cmaps)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously loaded data\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(14400, 420)\n",
      "(14400, 1)\n",
      "Cross-Validation data (X, y)\n",
      "(20, 420)\n",
      "(20, 1)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0.22299652 0.33979294 0.1771978  ... 0.2        0.82051282 0.71623349]\n",
      " [0.19860627 0.42500664 0.21350078 ... 0.2        0.88034188 0.74023576]\n",
      " [0.10452962 0.45872047 0.32790424 ... 0.1        0.70940171 0.71921602]\n",
      " [0.14982578 0.3480223  0.27138932 ... 0.3        0.74358974 0.78000284]\n",
      " [0.16724739 0.21210512 0.39776295 ... 0.1        0.62393162 0.7994603 ]]\n",
      "[[125.]\n",
      " [125.]\n",
      " [125.]\n",
      " [125.]\n",
      " [125.]]\n",
      "Cross-Validation data (X, y)\n",
      "[[0.49477352 0.73161667 0.75706436 ... 0.8        0.17094017 0.38488851]\n",
      " [0.45644599 0.54977436 0.57339089 ... 0.3        0.54700855 0.36940775]\n",
      " [0.51567944 0.54473055 0.66365777 ... 0.8        0.31623932 0.34995029]\n",
      " [0.31010453 0.54207592 0.44525118 ... 0.4        0.70940171 0.75770487]\n",
      " [0.41114983 0.55428723 0.43524333 ... 0.3        0.8034188  0.52648771]]\n",
      "[[ 11.]\n",
      " [ 94.]\n",
      " [ 10.]\n",
      " [138.]\n",
      " [121.]]\n",
      "Testing data (X, y)\n",
      "[[0.16027875 0.46217149 0.25843799 ... 0.4        0.57264957 0.65473654]\n",
      " [0.51219512 0.48075392 0.36185243 ... 0.2        0.57264957 0.52208493]\n",
      " [0.56097561 0.47411733 0.519427   ... 0.6        0.67521368 0.53941202]\n",
      " [0.41811847 0.60631802 0.50215856 ... 0.6        0.37606838 0.51683   ]\n",
      " [0.46341463 0.6469339  0.52158556 ... 0.5        0.52136752 0.73498083]]\n",
      "[[112.]\n",
      " [ 98.]\n",
      " [ 69.]\n",
      " [ 82.]\n",
      " [ 91.]]\n",
      "training with cv\n",
      "Train on 14400 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "14400/14400 [==============================] - 0s 10us/step - loss: 353.5572 - mean_squared_error: 295.9658 - val_loss: 537.0276 - val_mean_squared_error: 479.4394\n",
      "Epoch 2/10\n",
      "14400/14400 [==============================] - 0s 10us/step - loss: 352.5388 - mean_squared_error: 294.9856 - val_loss: 614.7391 - val_mean_squared_error: 557.1206\n",
      "Epoch 3/10\n",
      "14400/14400 [==============================] - 0s 12us/step - loss: 353.2340 - mean_squared_error: 295.7156 - val_loss: 498.1114 - val_mean_squared_error: 440.6219\n",
      "Epoch 4/10\n",
      "14400/14400 [==============================] - 0s 8us/step - loss: 351.6116 - mean_squared_error: 294.1526 - val_loss: 650.5151 - val_mean_squared_error: 592.9466\n",
      "Epoch 5/10\n",
      "14400/14400 [==============================] - 0s 11us/step - loss: 352.8336 - mean_squared_error: 295.4076 - val_loss: 548.0105 - val_mean_squared_error: 490.5583\n",
      "Epoch 6/10\n",
      "14400/14400 [==============================] - 0s 7us/step - loss: 350.7629 - mean_squared_error: 293.3584 - val_loss: 510.6809 - val_mean_squared_error: 453.3577\n",
      "Epoch 7/10\n",
      "14400/14400 [==============================] - 0s 7us/step - loss: 350.0384 - mean_squared_error: 292.7258 - val_loss: 559.2936 - val_mean_squared_error: 502.0268\n",
      "Epoch 8/10\n",
      "14400/14400 [==============================] - 0s 9us/step - loss: 350.3905 - mean_squared_error: 293.1697 - val_loss: 524.3652 - val_mean_squared_error: 467.1535\n",
      "Epoch 9/10\n",
      "14400/14400 [==============================] - 0s 8us/step - loss: 349.3542 - mean_squared_error: 292.2102 - val_loss: 502.4012 - val_mean_squared_error: 445.2840\n",
      "Epoch 10/10\n",
      "14400/14400 [==============================] - 0s 9us/step - loss: 349.0133 - mean_squared_error: 291.8943 - val_loss: 516.6741 - val_mean_squared_error: 459.6048\n",
      "100/100 [==============================] - 0s 72us/step\n",
      "\n",
      "Test Scores\n",
      "{'loss': 348.87940673828126, 'score_1': 291.81016723632814, 'rhs': array([4.66285161]), 'rmse': 17.04024647709064}\n"
     ]
    }
   ],
   "source": [
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0.2, reload_data=False)\n",
    "tModel.print_data()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(verbose=1)\n",
    "tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(selected_features)\n",
    "shape = len(selected_features)*window_size\n",
    "\n",
    "#Get tensorflow model\n",
    "model = get_tf_compiled_model(shape, \"regression\", output_size=1)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_tensorflow', model, lib_type='tensorflow', data_handler=dHandler_cmaps)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(tf_session=sess, get_minibatches_function_handle=aux_functions.get_minibatches, verbose=1)\n",
    "tModel.evaluate_model(['rhs', 'rmse'], tf_session=sess, round=2)\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TunableModel with classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "#Get keras model\n",
    "model = get_keras_compiled_model(input_dim, \"classification\", output_size=10)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelClassification('ModelMNIST_Keras', model, lib_type='keras', data_handler=dHandler_mnist)\n",
    "\n",
    "#tModel.data_handler.data_scaler = None\n",
    "#tModel.data_scaler = min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously loaded data\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(60000, 784)\n",
      "(60000, 10)\n",
      "Testing data (X, y)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Testing data (X, y)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "training without cv\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1075 - acc: 0.9702\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0990 - acc: 0.9731\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0920 - acc: 0.9751\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - acc: 0.9772\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0788 - acc: 0.9791\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0732 - acc: 0.9808\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0682 - acc: 0.9822\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0635 - acc: 0.9836: 1s - loss: 0.0\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0592 - acc: 0.9850\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0555 - acc: 0.9861\n",
      "10000/10000 [==============================] - 0s 28us/step\n",
      "\n",
      "Test Scores\n",
      "{'loss': 0.09461064505837857, 'score_1': 0.9706}\n"
     ]
    }
   ],
   "source": [
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0, reload_data=False)\n",
    "tModel.print_data()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(verbose=1)\n",
    "tModel.evaluate_model()\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "#Get tensorflow model\n",
    "model = get_tf_compiled_model(input_dim, \"classification\", output_size=10)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelClassification('ModelMNIST_Tensorflow', model, lib_type='tensorflow', data_handler=dHandler_mnist)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(tf_session=sess, get_minibatches_function_handle=aux_functions.get_minibatches, verbose=1)\n",
    "tModel.evaluate_model(tf_session=sess, metrics=['accuracy'])\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
